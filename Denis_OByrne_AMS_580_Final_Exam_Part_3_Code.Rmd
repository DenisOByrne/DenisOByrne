---
title: "AMS 580 Final Exam Part 3"
author: "Denis O'Byrne"
date: "5/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Part III. Unsupervised Learning with the GreatUnknown Data 
# Now we are back to the same GreatUnknown.csv data in Part I containing 12 predictors and one binary response variable y (= 0 or 1), which is the true class label.

# For this dataset, sensitivity is defined as a case labeled 1 being classified to label 1, while specificity is defined as a case labeled 0 being classified to label 0.

# 1.	Our goal is to predict the class label based on the 12 predictors given. Subsequently we wish to compare our data-driven clustering to that of the true class label (given by y). For each of the following three clustering methods (K-mean; Hierarchical: Ward, and, Average Linkage), we need you to: 
# a)	Perform the clustering analysis;
# b)	Draw scree-plot to see whether two clusters is reasonable or not; 
# c)	Show the 2D representation of the Cluster solution;
# d)	Build a confusion matrix to evaluate the clustering performance;
# For Hierarchical clustering, please also draw the dendrogram showing the two clusters obtained; 
# e)	Build a confusion matrix to compare the clustering results of the K-means and the Ward’s method;
# f)	Finally, make a comparison of all three clustering methods, and rank their performance for the given problem. 

# Setup
```{r}
library(tidyverse)
library(caret)
library(neuralnet)
library(randomForest)
library(rpart)
library(rattle)
library(MASS)
library(tidyverse)
library(glmnet)
library(leaps)
library(ggplot2)


gun<-read.csv("C:/Users/denis/Downloads/GreatUnknown.csv", header = TRUE)

head(gun)

dim(gun)

gun<-na.omit(gun)

dim(gun)
```


```{r}
y<-gun$y
x<-gun[-13]

x<-scale(x)

pc = princomp(x, cor = T)
# Scree-plot
library(factoextra)
fviz_eig(pc)
```

# K means
```{r}
k.means.fit <- kmeans(x, 2) 
library(cluster)
clusplot(x, k.means.fit$cluster, main='2D representation of the Cluster solution',color=TRUE, shade=TRUE,labels=2, lines=0)
table(k.means.fit$cluster, y)
```

# Kmeans Accuracy
```{r}
(2643+913)/(900+145+2643+913)
```

# H.Ward
```{r}
d <- dist(x, method = "euclidean")
H.fit <- hclust(d, method="ward.D")
plot(H.fit)
rect.hclust(H.fit, k=2, border="red")
groups <- cutree(H.fit, k=2)
clusplot(x, groups, main='2D representation of the Cluster solution',color=TRUE, shade=TRUE,labels=2, lines=0)
clusters = factor(groups, levels = 1:2)
table(y, clusters)
```

# Hward Accuracy
```{r}
(2621+754)/(2621+754+167+1059)
```

# H.Single
```{r}
H.fit <- hclust(d, method="single")
plot(H.fit)
rect.hclust(H.fit, k=2, border="red")
groups <- cutree(H.fit, k=2)
clusplot(x, groups, main='2D representation of the Cluster solution',color=TRUE, shade=TRUE,labels=2, lines=0)
clusters = factor(groups, levels = 1:2)
table(y, clusters)
```

# H.Single Accuracy
```{r}
(2788+1)/(1812+2788+1+0)
```

# H.Complete
```{r}
H.fit <- hclust(d, method="complete")
plot(H.fit)
rect.hclust(H.fit, k=2, border="red")
groups <- cutree(H.fit, k=2)
clusplot(x, groups, main='2D representation of the Cluster solution',color=TRUE, shade=TRUE,labels=2, lines=0)
clusters = factor(groups, levels = 1:2)
table(y, clusters)
```

# H.Complete Accuracy
```{r}
2785/(2785+1813+3+0)
```

# H.Average
```{r}
H.fit <- hclust(d, method="average")
plot(H.fit)
rect.hclust(H.fit, k=2, border="red")
groups <- cutree(H.fit, k=2)
clusplot(x, groups, main='2D representation of the Cluster solution',color=TRUE, shade=TRUE,labels=2, lines=0)
clusters = factor(groups, levels = 1:2)
table(y, clusters)
```

# H.Average Accuracy
```{r}
(2788+1)/(2788+1812+1)
```

# H.Centroid
```{r}
H.fit <- hclust(d, method="centroid")
plot(H.fit)
rect.hclust(H.fit, k=2, border="red")
groups <- cutree(H.fit, k=2)
clusplot(x, groups, main='2D representation of the Cluster solution',color=TRUE, shade=TRUE,labels=2, lines=0)
clusters = factor(groups, levels = 1:2)
table(y, clusters)
```

# H.Centroid Accuracy
```{r}
(2788+1)/(2788+1812+1)
```


# K-means clustering performed the best with a total accuracy of 0.7728755

# For this problem:
# K-means > HWard > HSingle = HAverage = HCentroid > HComplete


# 2.	Please write up the entire R code necessary to answer the following questions use the same data set: 
# a)	Please compute the Principal Components (PC’s) using the 12 predictors and print out a summary of the analysis – and in particular, please point out what percentage of the variations each PC would explain. 
# b)	Please make a biplot, which includes both the position of each sample in terms of PC1 and PC2 and will also show you how the initial variables map onto this. 
# c)	Now you will utilize the true class label (y) information binning the data into two groups: y = 0, and y = 1. You will visualize the biplot by setting the ellipse argument to be TRUE, which will draw an ellipse around each group.
# d)	Last but not the least, we shall print out the PC1 as linear combinations of the original variables. How would you interpret PC1 in terms of the original variables? 

```{r}
library(devtools)
library(ggbiplot)

y<-gun$y
x<-gun[-13]

x<-scale(x)
```
# (a)
```{r}
gun.pca = prcomp(x, center = TRUE,scale. = TRUE)
summary(gun.pca)
```

The proportions of variation by principal component are
# Pc1 = 13.39%
# PC2 = 10.81%
# PC3 = 8.91%
# PC4 = 8.411%
# PC5 = 8.285%
# PC6 = 8.072%
# PC7 = 7.929%
# PC8 = 7.795%
# PC9 = 7.591%
# PC10 = 6.999%
# PC11 = 6.638%
# PC12 = 5.169%

# (b)
```{r}
ggbiplot(gun.pca)

# just vectors
ggbiplot(gun.pca, alpha = 0)
```
# (c)
```{r}
ggbiplot(gun.pca, ellipse=TRUE, groups=y)
```

# (d)
```{r}
gun.pca$rotation[,1] #PC1
```

# The dependence of PC1 on the variables nonzero for all variables so it is not easily explained making it hard to interpret the model produced by the PCA.



