---
title: "AMS 580 Final Exam Part 2"
author: "Denis O'Byrne"
date: "5/13/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Part II. Regression Analyses & Variable Selections with the QuestionMark Data
# The QuestionMark.csv data contain 14 predictors and one continuous response variable y.  
# Our goal is to establish a regression model predicting the response variable using the predictors – and in particular, using various modern and classical techniques to perform variable selection.

# **Question 1**

# 1.	Please perform data cleaning by checking whether there are any missing values and if so, please delete observations with missing values. Please report how many observations with missing values we have in our dataset. Please use the random seed 123 to divide the data into 75% training and 25% testing.  

```{r}
library(tidyverse)
library(caret)
library(neuralnet)
library(randomForest)
library(rpart)
library(rattle)
library(MASS)
library(tidyverse)
library(glmnet)
library(leaps)
library(ggplot2)
library(caTools)


ques<-read.csv("C:/Users/denis/Downloads/QuestionMark.csv", header = TRUE)

head(ques)

dim(ques)

ques<-na.omit(ques)

dim(ques)
```
# There are ___ observations after removing missing data.

```{r}
set.seed(123)
set.seed(123) 
sample <- sample.split(ques$y, SplitRatio = .75)
train <- subset(ques, sample == TRUE)
test  <- subset(ques, sample == FALSE)
```

# **Question 2**

# 2.	Now we shall perform penalized regression analysis with three different methods.

# **Part a** Please first find the best Ridge Regression model using the training data. Please (a) find the best λ value through cross-validation and display this value; (b) display the coefficients of the fitted model; and (c) make prediction on the testing data, plot the observed response variable on the x-axis, and the estimated response variable on the Y-axis, and report the RMSE and the Coefficient of Determination R^2.

```{r}
x <- model.matrix(y~., train)[,-1]
y <- train$y
cv <- cv.glmnet(x, y, alpha = 0)
cv$lambda.min
```

## Best $\lambda$ for ridge regression is $\lambda_{min}=133.5985$

```{r}
model <- glmnet(x, y, alpha = 0, lambda = cv$lambda.min) # alpha=0: ridge
coef(model)


x.test <- model.matrix(y ~., test)[,-1]
predictions <- model %>% predict(x.test) %>% as.vector()

ggplot(data = test, aes(x = y, y = predictions)) + geom_point()


data.frame(
 RMSE = RMSE(predictions, test$y),
 Rsquare = R2(predictions, test$y)
)
```

## RMSE is 825.9467 and coefficient of determination is 0.603015.


# **Part b**	Please first find the best LASSO model using the training data. Please (a) find the best λ value through cross-validation and display this value; (b) display the coefficients of the fitted model; and (c) make prediction on the testing data, plot the observed response variable on the x-axis, and the estimated response variable on the Y-axis, and report the RMSE and the Coefficient of Determination R^2.

```{r}
cv <- cv.glmnet(x, y, alpha = 1)
cv$lambda.min
```

## Best $\lambda$ for LASSO is $\lambda_{min}=8.009019$

```{r}
model2 <- glmnet(x, y, alpha = 1, lambda = cv$lambda.min) # alpha=1: LASSO
coef(model)


x.test <- model.matrix(y ~., test)[,-1]
predictions <- model2 %>% predict(x.test) %>% as.vector()

ggplot(data = test, aes(x = y, y = predictions)) + geom_point()

data.frame(
 RMSE = RMSE(predictions, test$y),
 Rsquare = R2(predictions, test$y)
)
```

## RMSE is 887.0573 and coefficient of determination is 0.5726799.

# **Part c**	Please first find the best Elastic Net model using the training data. Please (a) find the best tuning parameter values through cross-validation and display these values; (b) display the coefficients of the fitted model; and (c) make prediction on the testing data, plot the observed response variable on the x-axis, and the estimated response variable on the Y-axis, and report the RMSE and the Coefficient of Determination R^2. 

```{r}
model <- train(
  y ~., data = train, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
)
model$bestTune

coef(model$finalModel, model$bestTune$lambda)

predictions <- model %>% predict(test)
data.frame(
  RMSE = RMSE(predictions, test$y),
  Rsquare = R2(predictions, test$y)
)
ggplot(data = test, aes(x = y, y = predictions)) + geom_point()

```

## RMSE is 896.5528 and coefficient of determination is 0.5697739.

# **Part d**	Please discuss which penalized regression method is the best for the QuestionMark.csv data, and why.

# The ridge regression model is best as it has the lowest RMSE and highest coefficient of determination.


# 3.	Now we shall perform variable selection using the best subset method, and the stepwise variable selection method. Please identify the best models (either the best subset model, or the best stepwise regression model) using the same training data as in Question 2 above, only

# (a)	We shall use ‘y’ as the response variables, and there are a total of 14 regressors to choose from. First, please use the R function regsubsets() [leaps package] for best-subset variable selection to identify different best models of different sizes ranging from 1 to 6.
```{r}
train.data<-train
test.data<-test

models <- regsubsets(y~., data = train.data, nvmax = 6) 
summary(models)
```

# (b)	Please use the 5-fold cross-validation to select the best overall model from all 6 best subset models identified in part (a) above. Please write down the equation of this best overall model.
```{r}
get_model_formula <- function(id, object, outcome){ 
  # get models data
  models <- summary(object)$which[id,-1]
  # Get outcome variable
  #form <- as.formula(object$call[[2]])
  #outcome <- all.vars(form)[1]
  # Get model predictors
  predictors <- names(which(models == TRUE))
  predictors <- paste(predictors, collapse = "+") 
  # Build model formula 
  as.formula(paste0(outcome, "~", predictors))
}

get_cv_error <- function(model.formula, data){
  set.seed(1)
  train.control <- trainControl(method = "cv", number = 5)
  cv <- train(model.formula, data = data, method = "lm",
              trControl = train.control)
  cv$results$RMSE
}

model.ids <- 1:6
cv.errors <-  map(model.ids, get_model_formula, models, "y") %>%
  map(get_cv_error, data = train.data) %>%
  unlist()
cv.errors
which.min(cv.errors)

bestsubcv<-lm(y ~ w1+w5+w6+w9+w10+w13, data = train.data)

preds<-predict(bestsubcv,test.data)

data.frame(
  RMSE = RMSE(preds, test$y),
  Rsquare = R2(preds, test$y)
)
```

# The overall best model is the model with 6 variables:

# y~w1+w5+w6+w9+w10+w13

# (c)	Please use the R function stepAIC() [MASS package] to identify the best model using the stepwise variable selection method. Please write down the equation of this best overall model.

```{r}
res.lm <- lm(y ~., data = train.data)
step <- stepAIC(res.lm, direction = "both", trace = FALSE) 
step
preds<-predict(step,test.data)

data.frame(
  RMSE = RMSE(preds, test$y),
  Rsquare = R2(preds, test$y)
)
```

# (d)	Compare to Question 2, the penalized regression methods above, which variable selection method is the overall best, based on what criterion (or criteria) of your choice? Please perform this comparison using the same testing data as in Question 2 above

# The Ridge regression method produced the best model with the lowest RMSE of 825.9467 and highest Rsquared of 0.603015 on new data




